{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce735581",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-12-15T16:25:29.044684Z",
     "iopub.status.busy": "2023-12-15T16:25:29.044114Z",
     "iopub.status.idle": "2023-12-15T16:25:32.414304Z",
     "shell.execute_reply": "2023-12-15T16:25:32.412840Z"
    },
    "papermill": {
     "duration": 3.380725,
     "end_time": "2023-12-15T16:25:32.417679",
     "exception": false,
     "start_time": "2023-12-15T16:25:29.036954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# IMPORT STUFF\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "import math\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import ast\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2840b206",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T16:25:32.431755Z",
     "iopub.status.busy": "2023-12-15T16:25:32.431122Z",
     "iopub.status.idle": "2023-12-15T16:25:32.437226Z",
     "shell.execute_reply": "2023-12-15T16:25:32.435933Z"
    },
    "papermill": {
     "duration": 0.015117,
     "end_time": "2023-12-15T16:25:32.439951",
     "exception": false,
     "start_time": "2023-12-15T16:25:32.424834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SET SOME GLOBAL VARIABLES\n",
    "QUERIES_PATH = \"/kaggle/input/dis2023-project1-data/queries.jsonl\"\n",
    "CORPUS_PATH = \"/kaggle/input/dis2023-project1-data/corpus.jsonl\"\n",
    "SAVEFILE_PATH = '/kaggle/input/savefile-txt/savefile.txt'\n",
    "SAVEFILE_QUERIES_PATH = '/kaggle/input/savefile-queries-txt/savefile_queries.txt'\n",
    "RESULTS_PATH = '/kaggle/working/submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95dd1b9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T16:25:32.452151Z",
     "iopub.status.busy": "2023-12-15T16:25:32.451188Z",
     "iopub.status.idle": "2023-12-15T16:25:32.464098Z",
     "shell.execute_reply": "2023-12-15T16:25:32.462908Z"
    },
    "papermill": {
     "duration": 0.021278,
     "end_time": "2023-12-15T16:25:32.466543",
     "exception": false,
     "start_time": "2023-12-15T16:25:32.445265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
      "Wall time: 7.87 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Commented as we have preprocessed files as part of the input\\n\\n# TOKENIZE CORPUS TEXTS AND SAVE TO SAVE-FILE\\nif not os.path.exists(SAVEFILE_PATH):\\n    with open(CORPUS_PATH) as f:\\n        raw_corpus = f.readlines()\\n        corpus = [json.loads(q) for q in raw_corpus]\\n        \\n    stemmer = PorterStemmer() \\n    trans = str.maketrans(string.punctuation, \" \" * len(string.punctuation))\\n\\n    f = open(SAVEFILE_PATH, \"a\")\\n    def tokenizeAndSave(nr, text):\\n        text = text.strip().translate(trans)\\n        tokens = nltk.word_tokenize(text)\\n        stemmed = [stemmer.stem(word.lower()) for word in tokens if word not in stopwords.words(\\'english\\')]\\n        f.write(str(nr) + \",\" + \\' \\'.join(stemmed) + \"\\n\")\\n\\n    for c in corpus:\\n        tokenizeAndSave(int(c[\"_id\"]), c[\"text\"])\\n    f.close()\\n\\n# TOKENIZE QUERIES AND SAVE TO SAVE-FILE\\nif not os.path.exists(SAVEFILE_QUERIES_PATH):\\n    queries = dict()\\n    with open(QUERIES_PATH) as f:\\n        raw_queries = f.readlines()\\n        for q in raw_queries:\\n            obj = json.loads(q)\\n            queries[int(obj[\"_id\"])] = obj[\"text\"]\\n            \\n    stemmer = PorterStemmer() \\n    trans = str.maketrans(string.punctuation, \" \" * len(string.punctuation))\\n    \\n    f = open(SAVEFILE_QUERIES_PATH, \"a\")\\n    def tokenizeAndSave(nr, text):\\n        text = text.strip().translate(trans)\\n        tokens = nltk.word_tokenize(text)\\n        stemmed = [stemmer.stem(word.lower()) for word in tokens if word not in stopwords.words(\\'english\\')]\\n        f.write(str(nr) + \",\" + \\' \\'.join(stemmed) + \"\\n\")\\n\\n    for q in queries.keys():\\n        tokenizeAndSave(q, queries[q])\\n    f.close()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# --------- PREPROCESSING START ---------\n",
    "\"\"\" Commented as we have preprocessed files as part of the input\n",
    "\n",
    "# TOKENIZE CORPUS TEXTS AND SAVE TO SAVE-FILE\n",
    "if not os.path.exists(SAVEFILE_PATH):\n",
    "    with open(CORPUS_PATH) as f:\n",
    "        raw_corpus = f.readlines()\n",
    "        corpus = [json.loads(q) for q in raw_corpus]\n",
    "        \n",
    "    stemmer = PorterStemmer() \n",
    "    trans = str.maketrans(string.punctuation, \" \" * len(string.punctuation))\n",
    "\n",
    "    f = open(SAVEFILE_PATH, \"a\")\n",
    "    def tokenizeAndSave(nr, text):\n",
    "        text = text.strip().translate(trans)\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        stemmed = [stemmer.stem(word.lower()) for word in tokens if word not in stopwords.words('english')]\n",
    "        f.write(str(nr) + \",\" + ' '.join(stemmed) + \"\\n\")\n",
    "\n",
    "    for c in corpus:\n",
    "        tokenizeAndSave(int(c[\"_id\"]), c[\"text\"])\n",
    "    f.close()\n",
    "\n",
    "# TOKENIZE QUERIES AND SAVE TO SAVE-FILE\n",
    "if not os.path.exists(SAVEFILE_QUERIES_PATH):\n",
    "    queries = dict()\n",
    "    with open(QUERIES_PATH) as f:\n",
    "        raw_queries = f.readlines()\n",
    "        for q in raw_queries:\n",
    "            obj = json.loads(q)\n",
    "            queries[int(obj[\"_id\"])] = obj[\"text\"]\n",
    "            \n",
    "    stemmer = PorterStemmer() \n",
    "    trans = str.maketrans(string.punctuation, \" \" * len(string.punctuation))\n",
    "    \n",
    "    f = open(SAVEFILE_QUERIES_PATH, \"a\")\n",
    "    def tokenizeAndSave(nr, text):\n",
    "        text = text.strip().translate(trans)\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        stemmed = [stemmer.stem(word.lower()) for word in tokens if word not in stopwords.words('english')]\n",
    "        f.write(str(nr) + \",\" + ' '.join(stemmed) + \"\\n\")\n",
    "\n",
    "    for q in queries.keys():\n",
    "        tokenizeAndSave(q, queries[q])\n",
    "    f.close()\n",
    "\"\"\"\n",
    "# --------- PREPROCESSING END ---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dbb0497",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T16:25:32.478625Z",
     "iopub.status.busy": "2023-12-15T16:25:32.477609Z",
     "iopub.status.idle": "2023-12-15T16:27:12.445877Z",
     "shell.execute_reply": "2023-12-15T16:27:12.444173Z"
    },
    "papermill": {
     "duration": 99.98185,
     "end_time": "2023-12-15T16:27:12.453177",
     "exception": false,
     "start_time": "2023-12-15T16:25:32.471327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 28s, sys: 7.93 s, total: 1min 36s\n",
      "Wall time: 1min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --------- LOADING DATA START ---------\n",
    "# LOAD TEXTS FROM FILES\n",
    "# load documents\n",
    "documents = {}\n",
    "with open(SAVEFILE_PATH) as f:\n",
    "    raw = f.readlines()\n",
    "    for line in raw:\n",
    "        nr, txt = line.split(',')\n",
    "        documents[int(nr)] = txt.split()\n",
    "\n",
    "# load queries\n",
    "queries = {}\n",
    "with open(SAVEFILE_QUERIES_PATH) as f:\n",
    "    raw = f.readlines()\n",
    "    for line in raw:\n",
    "        nr, txt = line.split(',')\n",
    "        queries[int(nr)] = txt.split()\n",
    "\n",
    "# TRAIN- AND TEST-FILES\n",
    "TASK1_TEST = '/kaggle/input/dis-project-1-text-retrieval/task1_test.tsv'\n",
    "TASK1_TRAIN = '/kaggle/input/dis-project-1-text-retrieval/task1_train.tsv'\n",
    "TASK2_TEST = '/kaggle/input/dis-project-1-text-retrieval/task2_test.tsv'\n",
    "TASK2_TRAIN = '/kaggle/input/dis-project-1-text-retrieval/task2_train.tsv'\n",
    "\n",
    "# read in train-data of task1\n",
    "tb = pd.read_table(TASK1_TRAIN)\n",
    "ind_gen = list(tb.iterrows())\n",
    "task1_traindata = {}\n",
    "for _, v in ind_gen:\n",
    "    task1_traindata[int(v['query-id'])] = int(v['corpus-id'])\n",
    "    \n",
    "# read in test-data of task1\n",
    "tb = pd.read_table(TASK1_TEST)\n",
    "ind_gen = list(tb.iterrows())\n",
    "task1_tests = []\n",
    "for _, v in ind_gen:\n",
    "    task1_tests.append((int(v['id']), int(v['query-id'])))\n",
    "\n",
    "# read in train-data of task2\n",
    "tb = pd.read_table(TASK2_TRAIN)\n",
    "ind_gen = list(tb.iterrows())\n",
    "task2_traindata = {}\n",
    "for _, v in ind_gen:\n",
    "    task2_traindata[int(v['query-id'])] = (ast.literal_eval(v['corpus-id']), ast.literal_eval(v['score']))\n",
    "\n",
    "# read in test-data of task2\n",
    "tb = pd.read_table(TASK2_TEST)\n",
    "ind_gen = list(tb.iterrows())\n",
    "task2_tests = []\n",
    "for _, v in ind_gen:\n",
    "    task2_tests.append((int(v['id']), int(v['query-id']), ast.literal_eval(v['corpus-id'])))\n",
    "# --------- LOADING DATA END ---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70328b51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T16:27:12.465032Z",
     "iopub.status.busy": "2023-12-15T16:27:12.464281Z",
     "iopub.status.idle": "2023-12-15T16:27:12.472505Z",
     "shell.execute_reply": "2023-12-15T16:27:12.471236Z"
    },
    "papermill": {
     "duration": 0.017186,
     "end_time": "2023-12-15T16:27:12.475075",
     "exception": false,
     "start_time": "2023-12-15T16:27:12.457889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open results file\n",
    "f = open(RESULTS_PATH, \"w\")\n",
    "f.write('id,corpus-id,score\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31e38682",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T16:27:12.488185Z",
     "iopub.status.busy": "2023-12-15T16:27:12.487736Z",
     "iopub.status.idle": "2023-12-15T16:36:16.804725Z",
     "shell.execute_reply": "2023-12-15T16:36:16.802122Z"
    },
    "papermill": {
     "duration": 544.327539,
     "end_time": "2023-12-15T16:36:16.808017",
     "exception": false,
     "start_time": "2023-12-15T16:27:12.480478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "CPU times: user 1min 7s, sys: 3.06 s, total: 1min 10s\n",
      "Wall time: 9min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --------- COMPUTATION TASK 1 START ---------\n",
    "# create the inverted index\n",
    "inv_idx = {}\n",
    "for nr in documents.keys():\n",
    "    for word in set(documents[nr]):\n",
    "        if word in inv_idx:\n",
    "            inv_idx[word].append(nr)\n",
    "        else:\n",
    "            inv_idx[word] = [nr]\n",
    "    \n",
    "# use inverted index to get n \"most important\" documents from corpus\n",
    "def doc_full_ranking(query, n):\n",
    "    indices = []\n",
    "    for word in query:\n",
    "        if word in inv_idx:\n",
    "            indices += inv_idx[word]\n",
    "    return [nr for nr,_ in Counter(indices).most_common(n)]\n",
    "\n",
    "# create a vocabulary from some texts\n",
    "def voc_from_txts(texts):\n",
    "    voc = set()\n",
    "    voc.update(*texts)\n",
    "    voc = list(voc)\n",
    "    return voc\n",
    "\n",
    "# calculate idf-values for all terms in a vocabulary\n",
    "def idf_values(voc, txts):\n",
    "    idf = {}\n",
    "    num_txts = len(txts)\n",
    "    for term in voc:\n",
    "        idf[term] = num_txts/sum(term in txt for txt in txts)\n",
    "    return idf\n",
    "\n",
    "# vectorize documents in \"doc\" list based on TF-IDF metric\n",
    "def vectorize(doc, voc, idf):\n",
    "    counts = Counter(doc)\n",
    "    max_count = counts.most_common(1)[0][1]\n",
    "    \n",
    "    vector = [0]*len(voc)\n",
    "    for i,term in enumerate(voc):\n",
    "        c = counts[term]\n",
    "        if c != 0:\n",
    "            vector[i] = idf[term] * c/max_count\n",
    "    return vector\n",
    "\n",
    "# calculate cosine simliarity between two vectors\n",
    "def cosine_similarity(v1,v2):\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    if sumxy == 0:\n",
    "            result = 0\n",
    "    else:\n",
    "            result = sumxy/math.sqrt(sumxx*sumyy)\n",
    "    return result\n",
    "\n",
    "# classic Vector Retrieval based on TF-IDF metric\n",
    "def vector_retrieval(query, doc_ids, n):\n",
    "    docs = [documents[did] for did in doc_ids]\n",
    "    voc = voc_from_txts(docs)\n",
    "    idfs = idf_values(voc, docs)\n",
    "    \n",
    "    vecs = [vectorize(s, voc, idfs) for s in docs]\n",
    "    query_vec = vectorize(query, voc, idfs)\n",
    "    \n",
    "    sims = [(d, cosine_similarity(query_vec, v)) for d, v in zip(doc_ids, vecs)]\n",
    "    sims = sorted(sims, key=lambda item: -item[1])\n",
    "    return [d for d,_ in sims[:n]]\n",
    "\n",
    "# main retrieval for a single query\n",
    "def retrieval(qid):\n",
    "    n_small = 10\n",
    "    n_big = 70\n",
    "    assert n_small <= n_big\n",
    "    query = queries[qid]\n",
    "    # Step 1: use inverted index\n",
    "    extracted_docs = doc_full_ranking(query, n_big)\n",
    "    # Step 2: use vector retrieval on documents extracted in phase 1\n",
    "    extracted_docs = vector_retrieval(query, extracted_docs, n_small)\n",
    "    return extracted_docs\n",
    "    \n",
    "\"\"\"# Training Data\n",
    "PROCESSES = 4\n",
    "keys = list(task1_traindata.keys())[:100]\n",
    "params = []\n",
    "for k in keys:\n",
    "    params.append((k,))\n",
    "with multiprocessing.Pool(PROCESSES) as pool:\n",
    "    results = [pool.apply_async(retrieval, p) for p in params]\n",
    "\n",
    "    correct = 0\n",
    "    for i, r in enumerate(results):\n",
    "        extracted_docs = r.get()\n",
    "        solution = task1_traindata[keys[i]]\n",
    "        if solution in extracted_docs:\n",
    "            correct += 1\n",
    "        if (i+1) % 10 == 0:\n",
    "            print((i+1), correct, \"->\", (correct / (i + 1)) * 100, \"%\")\n",
    "\"\"\"\n",
    "\n",
    "# Testing Data\n",
    "PROCESSES = 4\n",
    "params = []\n",
    "for i,qid in task1_tests:\n",
    "    params.append((i,(qid,)))\n",
    "with multiprocessing.Pool(PROCESSES) as pool:\n",
    "    results = [(i, pool.apply_async(retrieval, p)) for i,p in params]\n",
    "    for i, r in results:\n",
    "        extracted_docs = r.get()\n",
    "        # write to results file\n",
    "        f.write(f\"{i},\\\"{extracted_docs}\\\",-1\\n\")\n",
    "        if i % 500 == 0:\n",
    "            print(i)\n",
    "# --------- COMPUTATION TASK 1 END ---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ff670f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T16:36:16.824749Z",
     "iopub.status.busy": "2023-12-15T16:36:16.824255Z",
     "iopub.status.idle": "2023-12-15T16:36:36.467024Z",
     "shell.execute_reply": "2023-12-15T16:36:36.465246Z"
    },
    "papermill": {
     "duration": 19.655185,
     "end_time": "2023-12-15T16:36:36.470070",
     "exception": false,
     "start_time": "2023-12-15T16:36:16.814885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --------- COMPUTATION TASK 2 START ---------\n",
    "# map cosine-similarities to the set {0, 1, 2, 3} \n",
    "def float_to_rank(f):\n",
    "    if f > .1:\n",
    "        return 3\n",
    "    if f > .05:\n",
    "        return 2\n",
    "    if f > .01:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "# re-rank documents based on a query\n",
    "def topk_reranking(qid, corpusids):\n",
    "    reranking_docs = [documents[cid] for cid in corpusids]\n",
    "    reranking_voc = voc_from_txts(reranking_docs)\n",
    "    reranking_idf = idf_values(reranking_voc, reranking_docs)\n",
    "    \n",
    "    reranking_vecs = [vectorize(s, reranking_voc, reranking_idf) for s in reranking_docs]\n",
    "    query_vec = vectorize(queries[qid], reranking_voc, reranking_idf)\n",
    "    \n",
    "    sims = [float_to_rank(cosine_similarity(query_vec, v)) for v in reranking_vecs]\n",
    "    return sims\n",
    "\n",
    "\"\"\"# Training Data\n",
    "avg = 0\n",
    "for qid in task2_traindata.keys():\n",
    "    corpusids, scores = task2_traindata[qid]\n",
    "    res = topk_reranking(qid, corpusids)\n",
    "    # TODO: somehow this gives a completely wrong result :(\n",
    "    how_good = sum(a == b for a,b in zip(scores, res)) / len(scores)\n",
    "    avg += how_good\n",
    "# note: this gets an average of around 42%. if we just always return 0 we get >60% :(\n",
    "print(\"Average: \", (avg / len(task2_traindata.keys())) * 100, \"%\")\"\"\"\n",
    "\n",
    "# Testing Data\n",
    "for i, qid, corpusids in task2_tests:\n",
    "    res = topk_reranking(qid, corpusids)\n",
    "    f.write(f\"{i},-1,\\\"{res}\\\"\\n\")\n",
    "# --------- COMPUTATION TASK 2 END ---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da52a075",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T16:36:36.486556Z",
     "iopub.status.busy": "2023-12-15T16:36:36.486073Z",
     "iopub.status.idle": "2023-12-15T16:36:36.492698Z",
     "shell.execute_reply": "2023-12-15T16:36:36.491085Z"
    },
    "papermill": {
     "duration": 0.019146,
     "end_time": "2023-12-15T16:36:36.495939",
     "exception": false,
     "start_time": "2023-12-15T16:36:36.476793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Close results file\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 6624299,
     "sourceId": 60797,
     "sourceType": "competition"
    },
    {
     "datasetId": 3780771,
     "sourceId": 6544699,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3859468,
     "sourceId": 6694027,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3887449,
     "sourceId": 6752857,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3887450,
     "sourceId": 6752863,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30558,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 674.884657,
   "end_time": "2023-12-15T16:36:39.429950",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-15T16:25:24.545293",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
