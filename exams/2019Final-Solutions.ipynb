{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Information Systems\n",
    "\n",
    "***Final Exam, Spring Semester, 2019***\n",
    "\n",
    "The exam will be held on your computer, but digital communication by any means is **strictly prohibited**. You are allowed, though, to use *StackOverflow* or similar websites to resolve syntax-related Python errors. \n",
    "The following materials are also allowed: exercise sheets and solutions, past exams with your own solution, personally written notes and personally collected documentation.\n",
    "By participating in this exam you **agree to these conditions**.\n",
    "\n",
    "These are the instructions for the exam:\n",
    "\n",
    "- You are not allowed to leave the examination room in the first 20 and the last 15 minutes of the exam.\n",
    "- The quiz will remain open **only for the first 2 hours** of the exam to avoid network congestion.\n",
    "- **30 minutes** before the end of the exam we will announce a password to upload your jupyter notebook on Moodle.\n",
    "- It is not recommended to leave the exam before the password is published. If you need to leave earlier, contact us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#0-Rename-your-notebook\" data-toc-modified-id=\"0-Rename-your-notebook-0\">0 Rename your notebook</a></span></li><li><span><a href=\"#1-Multiple-Choice-Questions\" data-toc-modified-id=\"1-Multiple-Choice-Questions-1\">1 <a href=\"https://moodle.epfl.ch/mod/quiz/view.php?id=1026316\" target=\"_blank\">Multiple Choice Questions</a></a></span></li><li><span><a href=\"#2-Implementing-a-Rule-Based-Approach-for-Entity-Disambiguation\" data-toc-modified-id=\"2-Implementing-a-Rule-Based-Approach-for-Entity-Disambiguation-2\">2 Implementing a Rule-Based Approach for Entity Disambiguation</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.1-Learning-rules\" data-toc-modified-id=\"2.1-Learning-rules-2.1\">2.1 Learning rules</a></span></li><li><span><a href=\"#2.2-Finding-new-rules-using-bootstrapping\" data-toc-modified-id=\"2.2-Finding-new-rules-using-bootstrapping-2.2\">2.2 Finding new rules using bootstrapping</a></span></li></ul></li><li><span><a href=\"#3-Academic-Communities\" data-toc-modified-id=\"3-Academic-Communities-3\">3 Academic Communities</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.1-Modularity\" data-toc-modified-id=\"3.1-Modularity-3.1\">3.1 Modularity</a></span></li><li><span><a href=\"#3.2-Community-Detection\" data-toc-modified-id=\"3.2-Community-Detection-3.2\">3.2 Community Detection</a></span></li><li><span><a href=\"#3.3-Community-Influencers\" data-toc-modified-id=\"3.3-Community-Influencers-3.3\">3.3 Community Influencers</a></span></li><li><span><a href=\"#3.4-Connectivity-Based-Community-Ranking\" data-toc-modified-id=\"3.4-Connectivity-Based-Community-Ranking-3.4\">3.4 Connectivity-Based Community Ranking</a></span></li><li><span><a href=\"#3.5-Personalized-Community-Ranking\" data-toc-modified-id=\"3.5-Personalized-Community-Ranking-3.5\">3.5 Personalized Community Ranking</a></span></li><li><span><a href=\"#3.6-TF-IDF-Community-Ranking\" data-toc-modified-id=\"3.6-TF-IDF-Community-Ranking-3.6\">3.6 TF-IDF Community Ranking</a></span></li><li><span><a href=\"#3.7-Rankings-Correlation\" data-toc-modified-id=\"3.7-Rankings-Correlation-3.7\">3.7 Rankings Correlation</a></span></li></ul></li><li><span><a href=\"#Removed\" data-toc-modified-id=\"Removed-4\">Removed</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-is-a-profile-based-recommendation-ranking-for-a-user?\" data-toc-modified-id=\"What-is-a-profile-based-recommendation-ranking-for-a-user?-4.1\">What is a profile-based recommendation ranking for a user?</a></span></li><li><span><a href=\"#What-is-the-ranking-of-the-communities-based-on-the-importance-of-their-nodes?\" data-toc-modified-id=\"What-is-the-ranking-of-the-communities-based-on-the-importance-of-their-nodes?-4.2\">What is the ranking of the communities based on the importance of their nodes?</a></span></li><li><span><a href=\"#What-is-the-ranking-of-the-communities-based-on-their-intra-connectivity?\" data-toc-modified-id=\"What-is-the-ranking-of-the-communities-based-on-their-intra-connectivity?-4.3\">What is the ranking of the communities based on their intra-connectivity?</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Rename your notebook\n",
    "Replace SciperNo with your **personal SCIPER Number**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 [Multiple Choice Questions](https://moodle.epfl.ch/mod/quiz/view.php?id=1026316)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Implementing a Rule-Based Approach for Entity Disambiguation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Learning rules\n",
    "\n",
    "We would like to develop a rule-based approach to disambiguate the meaning of the term \"Apple\" in sentences into the two possible meanings **Tech** and **Fruit**. The rules use keyphrases i.e., n-grams of words, as features in their conditions. In the approach **only the top-k keyphrases** are considered. Before forming keyphrases, the words are stemmed, stopwords are not considered and uppercase words are converted to lowercase.\n",
    "\n",
    "An example of a possible rule would be:\n",
    "\n",
    "\tif \"fruit\" and \"facebook\" then \"Tech\"\n",
    "\n",
    "meaning that if a sentence that contains the term \"Apple\" also contains the unigrams \"fruit\" and \"facebook\" then the meaning of \"Apple\" should be the Apple technology company (**Tech**).\n",
    "\n",
    "More generally we write such rules as:\n",
    "\n",
    "\t{k1,...,kn} -> Tech or {k1,...,kn} -> Fruit\n",
    "    \n",
    "where k1,..,kn are arbitrary word n-grams and a set {k1,...,kn} is considered as an item set.\n",
    "\n",
    "As **training data** we obtain 10 sentences containing \"Apple\" that have been labelled by their meaning as T(ech) or F(ruit).\n",
    "\n",
    "    An apple is a fruit and good for health. (F)\n",
    "    Apple is a tech company. (T)\n",
    "    Tech companies like Apple and Facebook have big data centers (T)\n",
    "    For maintaining health eat one apple a day. (F)\n",
    "    A new Apple data center has been opened next to the one of Facebook. (T)\n",
    "    Apple has sold 1 million units in one day. (T)\n",
    "    Fruits, like apples and pears, are contaminated with pesticides.  (F)\n",
    "    A fruit salad contains apples, bananas and pears.  (F)\n",
    "    I saw a new apple recipe on Facebook.  (F)\n",
    "    Apple is increasingly processing health data. (T)\n",
    "\n",
    "**Stopwords** are: an, is, a, and, for, like, and, have, has, been, in, are, with, on, by, as, may, one, to, the, of.\n",
    "\n",
    "We define the **confidence** of a rule {k1,...,kn} -> T as the fraction between the number of sentences that contain all keyphrases k1,...,kn with label T over the number of sentences with all keyphrases k1,...,kn. Similar definition for {k1,...,kn} -> F.\n",
    "\n",
    "We define the **support** of an itemset as the number of sentences that contain all the n-grams in the itemset.\n",
    "\n",
    "**Questions:** \n",
    "1. Without considering the word \"apple\", determine the keyphrases with minimum document frequency of 2.\n",
    "2. Determine all itemsets of keyphrases with a minimum support of 2.\n",
    "3. Determine all rules that have a minimum support of 2 and a minimum confidence of 70%. Provide for each rule its support and confidence values. \n",
    "4. Explain how the apriori property could be exploited in optimizing the computation of those itemsets for composite keyphrases.\n",
    "\n",
    "Hint: the following are the top-10 document frequencies of terms.\n",
    "\n",
    "| Term        | Frequency           | \n",
    "| ------------- |:-------------:| \n",
    "|fruit | 3|\n",
    "|health | 3|\n",
    "|data | 3|\n",
    "|facebook | 3|\n",
    "|tech | 2|\n",
    "|compani | 2|\n",
    "|center | 2|\n",
    "|day | 2|\n",
    "|new | 2|\n",
    "|pear |2|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***ANSWER:***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Finding new rules using bootstrapping\n",
    "\n",
    "We are using the rules we have learnt now to find new sentences on Apple (e.g. using Google search) to increase our training data and thus learning new rules.\n",
    "\n",
    "Performing this approach we found 10 more sentences:\n",
    "\n",
    "    Apple and Facebook data center filed plans to expand data center operations in Prineville (T) \n",
    "    Apple, Facebook, Google Asked to Pay for Wind Parks in Denmark (T) \n",
    "    Google, Facebook and Apple lead on green data centers (T)\n",
    "    Apple to contest patent filed by Google. (T) \n",
    "    Green data centers are the new priority for Apple and Google. (T)\n",
    "    Green apples are an ideal ingredient for fruit salad. (F)\n",
    "    Apple Pears have been described as the hottest new item since the Kiwi (F)\n",
    "    Join Facebook to connect with Apples Pear and others you may know (F)\n",
    "    An apple is a sweet, edible fruit produced by an apple tree (F)\n",
    "    An apple a day keeps the doctor away (F)\n",
    "\n",
    "**Questions:** \n",
    "\n",
    "1. Given that the size of the training set has increased, should confidence and/or support threshold be adapted and how?\n",
    "2. Identify all new rules with adapted confidence and support threshold that can be derived from the training set that has been enlarged by the examples that have been retrieved using a bootstrapping approach.\n",
    "3. Once you found a new rule, explain how you would proceed to verify that the new rule does not introduce semantic shift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***ANSWER:***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Academic Communities\n",
    "The dataset you are about to explore is a snapshot of the **retweet network** among official accounts of universities and academic institutes. The **nodes** of the network are twitter handles (usernames), while the **edges** are attributed with a **label**, which was extracted from the topic of the tweet, and a **weight**, which depicts the popularity of the original tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(pd.read_csv('uni_network.csv'), 'Source', 'Target', edge_attr=['Label', 'Weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Modularity\n",
    "- Implement the modularity metric for communities.\n",
    "- Use the toy example and the assertion below to validate your results.\n",
    "- Hint: You can reuse code from the exercise sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def communities_modularity(G, nodes_community):\n",
    "    ''' input: G:nx.Graph \n",
    "               nodes_community:{node_id:community_id}\n",
    "       output: Q (modularity metric)\n",
    "    '''\n",
    "    Q = 0\n",
    "    \n",
    "    # Add your code here\n",
    "    m = len(G.edges)\n",
    "    for node_i in G.nodes:\n",
    "        for node_j in G.nodes:\n",
    "            if nodes_community[node_i] == nodes_community[node_j]:\n",
    "                Q += G.number_of_edges(node_i, node_j) - G.degree[node_i]*G.degree[node_j]/(2*m)\n",
    "    Q = Q/(2*m)\n",
    "    # Add your code here\n",
    "\n",
    "    return Q \n",
    "\n",
    "Q = communities_modularity(nx.Graph([(1, 2), (2, 3), (3, 1)]), {1:'a', 2:'a', 3:'a'})\n",
    "assert (round(Q, 4) == 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Community Detection\n",
    "\n",
    "- In the following cells you are given two snippets of code for community detection:\n",
    "    - The first approach (`network_structure_communities`) computes communities based on the **network structure**. The algorithm that is used is the *Clauset-Newman-Moore greedy modularity maximization* and is similar to the *Louvain modularity maximization*.\n",
    "    - The second approach (`nodes_label_communities`) computes communities based on the **labels of the graph nodes**. The label of a node is determined by the dominant label of its edges. All nodes with the same label belong to the same community.\n",
    "\n",
    "- Based on the statistics that we present below, discuss what are the **pros and cons** of each approach.\n",
    "- Note: You don't have to code for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_structure_communities(G):\n",
    "    nodes_community = community.greedy_modularity_communities(G, weight='Weight')\n",
    "    nodes_community = {i:indx for indx, c in enumerate(nodes_community) for i in c}\n",
    "    nodes_community = {n:nodes_community[n] for n in G.nodes}\n",
    "    return nodes_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nodes_label_communities(G):\n",
    "    nodes_community = {}\n",
    "    for n in G.nodes():\n",
    "        labels = [[e[2]['Label']]*e[2]['Weight'] for e in G.edges(n, data=True)]\n",
    "        labels = [item for l in labels for item in l]\n",
    "        nodes_community[n] = max(set(labels), key = labels.count)\n",
    "    return nodes_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Average Community Size</th>\n",
       "      <th>Communities Count</th>\n",
       "      <th>Modularity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1st approach</th>\n",
       "      <td>3.192053</td>\n",
       "      <td>453</td>\n",
       "      <td>0.919974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2nd approach</th>\n",
       "      <td>48.200000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.688264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Average Community Size  Communities Count  Modularity\n",
       "                                                                   \n",
       "1st approach                3.192053                453    0.919974\n",
       "2nd approach               48.200000                 30    0.688264"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_approaches(G):\n",
    "    nodes_community_1 = network_structure_communities(G)\n",
    "    nodes_community_2 = nodes_label_communities(G)\n",
    "    communities_count_1 = len(set(nodes_community_1.values()))\n",
    "    communities_count_2 = len(set(nodes_community_2.values()))\n",
    "\n",
    "    return pd.DataFrame.from_dict([{'':'1st approach', 'Communities Count':communities_count_1, 'Average Community Size':len(G.nodes())/communities_count_1, 'Modularity':communities_modularity(G, nodes_community_1)},\\\n",
    "                                   {'':'2nd approach', 'Communities Count':communities_count_2, 'Average Community Size':len(G.nodes())/communities_count_2, 'Modularity':communities_modularity(G, nodes_community_2)}]).set_index('')\n",
    "\n",
    "compare_approaches(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***ANSWER:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following tasks we will use the communities that are detected by the **second approach**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_community = nodes_label_communities(G)\n",
    "communities = list(set(nodes_community.values()))\n",
    "communities_count = len(communities)\n",
    "default_ranking = {c:0.0 for c in communities}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Community Influencers\n",
    "- Isolate each community from the graph. \n",
    "- Select the node with the **maximum pagerank** within each community as the **influencer** of that community.\n",
    "- Break ties arbitrarily.\n",
    "- Hint: Useful functions: `nx.pagerank()`, `G.subgraph()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('architecture', 'UniofGreenwich'),\n",
       " ('artificial intelligence', 'HECParis'),\n",
       " ('astronomy', 'HistAstro'),\n",
       " ('biological', 'thatsfarming'),\n",
       " ('cancer', 'IrishCancerSoc'),\n",
       " ('cell', 'GilesYeo'),\n",
       " ('chemistry', 'UniofNottingham'),\n",
       " ('climate change', 'meganrowling'),\n",
       " ('computer science', 'UniofHerts'),\n",
       " ('criminology', 'ARUEngagement'),\n",
       " ('cyber security', 'eevriviades'),\n",
       " ('diabetes', 'UoMNews'),\n",
       " ('drones', 'HarperAdamsUni'),\n",
       " ('economics', 'EmPoints'),\n",
       " ('entrepreneurship', 'RoyalAgUni'),\n",
       " ('geography', 'drheatherprice'),\n",
       " ('healthcare', 'VoiceofNursing_'),\n",
       " ('infection', 'UofGMVLS'),\n",
       " ('innovation', 'tcddublin'),\n",
       " ('medicine', 'learnhospice'),\n",
       " ('neuroscience', 'ArtScienceDoc'),\n",
       " ('pathways', 'RyanAcademy'),\n",
       " ('patients', 'WPXI'),\n",
       " ('physicist', 'royalsociety'),\n",
       " ('physics', 'SymeonDagkas'),\n",
       " ('psychology', 'BPSOfficial'),\n",
       " ('robotics', 'Phil_Baty'),\n",
       " ('social media', 'NYDailyNews'),\n",
       " ('sustainability', 'UniversityLeeds'),\n",
       " ('therapy', 'UBICBrighton')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def community_influencers(G, nodes_community, communities, communities_count):\n",
    "    ''' input: G:nx.Graph \n",
    "               nodes_community:{node_id:community_id}\n",
    "               communities:[community_ids]\n",
    "               community_count:int\n",
    "       output: influencers:{community_id:node_id}\n",
    "    '''\n",
    "    influencers = {}\n",
    "    \n",
    "    # Add your code here\n",
    "    \n",
    "    for c in communities:\n",
    "        nodes = [n for n in G.nodes if nodes_community[n]==c]\n",
    "        pr = nx.pagerank(G.subgraph(nodes))\n",
    "        influencers[c] = max(pr, key=pr.get)\n",
    "\n",
    "    # Add your code here\n",
    "    \n",
    "    return influencers\n",
    "\n",
    "influencers = community_influencers(G, nodes_community, communities, communities_count)\n",
    "sorted(influencers.items(), key=lambda x: x[0]) if influencers != {} else None # prints sorted results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Connectivity-Based Community Ranking\n",
    "- Compute a meta graph where nodes are communities and edges denote inter-connections across communities. \n",
    "- Add the weights of the inter-connections as weights to the edges.\n",
    "- Compute `pagerank` on the meta graph.\n",
    "- Hint: `w_matrix` is the confusion matrix of the weights among the communities. `w_matrix` is not symmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('innovation', 0.1175316220359166),\n",
       " ('artificial intelligence', 0.1157057509495049),\n",
       " ('psychology', 0.07266281991431879),\n",
       " ('entrepreneurship', 0.0565836336445204),\n",
       " ('sustainability', 0.05043936029740328),\n",
       " ('computer science', 0.044925965192617864),\n",
       " ('physics', 0.04192640288042445),\n",
       " ('climate change', 0.03774773967426391),\n",
       " ('chemistry', 0.036150780561586215),\n",
       " ('architecture', 0.03318939114735931),\n",
       " ('economics', 0.02994993856239657),\n",
       " ('social media', 0.029270293765880844),\n",
       " ('patients', 0.02668015432185533),\n",
       " ('cyber security', 0.026417237694418127),\n",
       " ('biological', 0.025453215452582906),\n",
       " ('healthcare', 0.024964310859068403),\n",
       " ('robotics', 0.023854191645331445),\n",
       " ('medicine', 0.022594739356807246),\n",
       " ('drones', 0.02209027906346713),\n",
       " ('geography', 0.020717966222541007),\n",
       " ('therapy', 0.020215995737512066),\n",
       " ('criminology', 0.018981304860549143),\n",
       " ('diabetes', 0.01828139923522191),\n",
       " ('pathways', 0.017711351235234492),\n",
       " ('cell', 0.016672897412689864),\n",
       " ('cancer', 0.013771402929793076),\n",
       " ('physicist', 0.010622217148478514),\n",
       " ('astronomy', 0.009921512456962477),\n",
       " ('neuroscience', 0.00829679545549693),\n",
       " ('infection', 0.006669330285796837)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def connectivity_ranking(G, nodes_community, communities, communities_count):\n",
    "    ''' input: G:nx.Graph \n",
    "               nodes_community:{node_id:community_id}\n",
    "               communities:[community_ids]\n",
    "               community_count:int\n",
    "       output: communities_ranking:{community_id:ranking}\n",
    "\n",
    "    '''\n",
    "    communities_ranking = default_ranking.copy()\n",
    "    \n",
    "    meta_G = nx.Graph()\n",
    "    w_matrix = {c2:{c1:0 for c1 in communities} for c2 in communities}\n",
    "    for (n1, n2, weight) in G.edges(data='Weight'):\n",
    "        w_matrix[nodes_community[n1]][nodes_community[n2]] += weight\n",
    "\n",
    "    # Add your code here\n",
    "    \n",
    "    for c1 in communities:\n",
    "        for c2 in communities:\n",
    "            if (c1 < c2):\n",
    "                weight = w_matrix[c1][c2] + w_matrix[c2][c1]\n",
    "                meta_G.add_edge(c1, c2, weight=weight)\n",
    "\n",
    "    communities_ranking = nx.pagerank(meta_G)\n",
    "    \n",
    "    # Add your code here\n",
    "    \n",
    "    return communities_ranking\n",
    "\n",
    "connectivity_ranking = connectivity_ranking(G, nodes_community, communities, communities_count)\n",
    "sorted(connectivity_ranking.items(), key=lambda x: x[1], reverse=True) if connectivity_ranking != default_ranking else None # prints sorted results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Personalized Community Ranking\n",
    "- Compute, for each community, the **personalized pagerank** of their **influencers**, where the source nodes belong to the community **artificial intelligence**.\n",
    "- Hint: Useful function: `nx.pagerank()`; `personalization` parameter defines the probability of random jumping to a source node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('artificial intelligence', 0.011076392257409935),\n",
       " ('computer science', 0.005304729258854428),\n",
       " ('drones', 0.002652884097355944),\n",
       " ('psychology', 0.00038444405302006843),\n",
       " ('innovation', 1.8889741877389747e-06),\n",
       " ('physicist', 1.7810328800477428e-06),\n",
       " ('physics', 1.5584037860454496e-06),\n",
       " ('architecture', 1.0389358574629546e-06),\n",
       " ('pathways', 1.0389358574629546e-06),\n",
       " ('economics', 1.0389358574629546e-06),\n",
       " ('climate change', 1.0389358574629546e-06),\n",
       " ('astronomy', 1.0389358574629546e-06),\n",
       " ('medicine', 1.0389358574629546e-06),\n",
       " ('biological', 1.0389358574629546e-06),\n",
       " ('chemistry', 1.0389358574629546e-06),\n",
       " ('geography', 1.0389358574629546e-06),\n",
       " ('entrepreneurship', 1.0389358574629546e-06),\n",
       " ('patients', 1.0389358574629546e-06),\n",
       " ('neuroscience', 1.0389358574629546e-06),\n",
       " ('cancer', 1.0389358574629546e-06),\n",
       " ('therapy', 1.0389358574629546e-06),\n",
       " ('cyber security', 1.0389358574629546e-06),\n",
       " ('diabetes', 1.0389358574629546e-06),\n",
       " ('robotics', 1.0389358574629546e-06),\n",
       " ('infection', 1.038935857462954e-06),\n",
       " ('cell', 1.0389358574629538e-06),\n",
       " ('healthcare', 1.0389358574629533e-06),\n",
       " ('sustainability', 1.0389358574629533e-06),\n",
       " ('social media', 1.038935857462952e-06),\n",
       " ('criminology', 8.31148686598775e-07)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def personalized_ranking(G, nodes_community, communities, communities_count):\n",
    "    ''' input: G:nx.Graph \n",
    "               nodes_community:{node_id:community_id}\n",
    "               communities:[community_ids]\n",
    "               community_count:int\n",
    "       output: communities_ranking:{community_id:ranking}\n",
    "\n",
    "    '''\n",
    "    communities_ranking = default_ranking.copy()\n",
    "    influencers = community_influencers(G, nodes_community, communities, communities_count)\n",
    "\n",
    "    # Add your code here\n",
    "    \n",
    "    def personalization(nodes_community, community):\n",
    "        count = list(nodes_community.values()).count(community)\n",
    "        return {n:1/count if nodes_community[n] == community else 0 for n in nodes_community.keys()}\n",
    "\n",
    "    personalization = personalization(nodes_community, 'artificial intelligence')\n",
    "    for c in communities:\n",
    "        ppr = nx.pagerank(G, personalization=personalization)\n",
    "        communities_ranking[c] = ppr[influencers[c]]\n",
    "\n",
    "    # Add your code here\n",
    "\n",
    "    return communities_ranking\n",
    " \n",
    "personalized_ranking = personalized_ranking(G, nodes_community, communities, communities_count)\n",
    "sorted(personalized_ranking.items(), key=lambda x: x[1], reverse=True) if personalized_ranking != default_ranking else None # prints sorted results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 TF-IDF Community Ranking\n",
    "- Treat each community as a document.\n",
    "- Treat **artificial intelligence** as a query.\n",
    "- Rank the documents (communities) based on their similarity to the query.\n",
    "- Hint: Useful function: `cosine_similarity()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('artificial intelligence', 0.7068180786730306),\n",
       " ('computer science', 0.1363312004264351),\n",
       " ('drones', 0.0736540162872044),\n",
       " ('diabetes', 0.039546236270366075),\n",
       " ('psychology', 0.026595405327496296),\n",
       " ('physics', 0.025330985903952923),\n",
       " ('entrepreneurship', 0.024021069069528018),\n",
       " ('innovation', 0.023216563644012467),\n",
       " ('chemistry', 0.017660462096875655),\n",
       " ('architecture', 0.015594934503474007),\n",
       " ('cell', 0.00959421139643584),\n",
       " ('climate change', 0.009562666055153999),\n",
       " ('social media', 0.0008954193092398574),\n",
       " ('healthcare', 0.0),\n",
       " ('pathways', 0.0),\n",
       " ('sustainability', 0.0),\n",
       " ('economics', 0.0),\n",
       " ('astronomy', 0.0),\n",
       " ('medicine', 0.0),\n",
       " ('criminology', 0.0),\n",
       " ('biological', 0.0),\n",
       " ('geography', 0.0),\n",
       " ('patients', 0.0),\n",
       " ('infection', 0.0),\n",
       " ('neuroscience', 0.0),\n",
       " ('cancer', 0.0),\n",
       " ('therapy', 0.0),\n",
       " ('physicist', 0.0),\n",
       " ('cyber security', 0.0),\n",
       " ('robotics', 0.0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tfidf_ranking(G, nodes_community, communities, communities_count):\n",
    "    ''' input: G:nx.Graph \n",
    "               nodes_community:{node_id:community_id}\n",
    "               communities:[community_ids]\n",
    "               community_count:int\n",
    "       output: communities_ranking:{community_id:ranking}\n",
    "\n",
    "    '''\n",
    "    communities_ranking = default_ranking.copy()\n",
    "\n",
    "    documents = [''] * communities_count\n",
    "    query = ['artificial inteligence']\n",
    "    \n",
    "    for n in nodes_community:\n",
    "        labels = ''.join([str(e[2]['Label'] + ' ')*e[2]['Weight'] for e in G.edges(n, data=True)])\n",
    "        documents[communities.index(nodes_community[n])] += labels\n",
    "\n",
    "    # Add your code here\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    D = vectorizer.fit_transform(documents)\n",
    "    Q = vectorizer.transform(query)\n",
    "    \n",
    "    sim = cosine_similarity(D, Q)\n",
    "    for i, c in enumerate(communities):\n",
    "        communities_ranking[c] = sim[i][0]\n",
    "\n",
    "    # Add your code here\n",
    "    \n",
    "    return communities_ranking\n",
    "\n",
    "tfidf_ranking = tfidf_ranking(G, nodes_community, communities, communities_count)\n",
    "sorted(tfidf_ranking.items(), key=lambda x: x[1], reverse=True) if tfidf_ranking != default_ranking else None # prints sorted results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Rankings Correlation\n",
    "- Consider `personalized_ranking`, `tfidf_ranking` and `AVG(connectivity_ranking, tfidf_ranking)`.\n",
    "- Compute the `3x3` correlation matrix.\n",
    "- Discuss which rankings correlate more and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.95496264, 0.9381539 ],\n",
       "       [0.95496264, 1.        , 0.9894849 ],\n",
       "       [0.9381539 , 0.9894849 , 1.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_correlation(connectivity_ranking, personalized_ranking, tfidf_ranking):    \n",
    "    correlation = np.zeros([3,3])\n",
    "    \n",
    "    connectivity_ranking = [r[1] for r in sorted(connectivity_ranking.items(), key=lambda x: x[0])]\n",
    "    personalized_ranking = [r[1] for r in sorted(personalized_ranking.items(), key=lambda x: x[0])]\n",
    "    tfidf_ranking = [r[1] for r in sorted(tfidf_ranking.items(), key=lambda x: x[0])]\n",
    "    avg_connectivity_tfidf_ranking = [sum(x)/2 for x in zip(connectivity_ranking, tfidf_ranking)]\n",
    "    \n",
    "    # Add your code here    \n",
    "    \n",
    "    correlation = np.corrcoef(np.array([personalized_ranking, tfidf_ranking, avg_connectivity_tfidf_ranking]))\n",
    "    \n",
    "    # Add your code here    \n",
    "\n",
    "    return correlation\n",
    "\n",
    "compute_correlation(connectivity_ranking, personalized_ranking, tfidf_ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***ANSWER:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a profile-based recommendation ranking for a user?\n",
    "- Consider the user with the label **artificial intelligence**.\n",
    "- Recommend users that have similar labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CyclingSurgeon', 0.7071067811865476),\n",
       " ('ManiraAhmad', 0.7071067811865476),\n",
       " ('DigiCatapult', 0.7071067811865476),\n",
       " ('StuffandStories', 0.7071067811865476),\n",
       " ('victoria_holt', 0.7071067811865476)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def profile_based_recommendation(G, nodes_community, communities, communities_count):\n",
    "    ''' input: G:nx.Graph \n",
    "               nodes_community:{node_id:community_id}\n",
    "               communities:[community_ids]\n",
    "               community_count:int\n",
    "       output: users_ranking:{user_id:ranking}\n",
    "\n",
    "    '''\n",
    "    users_ranking = {n:0.0 for n in nodes_community.keys()}\n",
    "\n",
    "    # Add your code here\n",
    "    \n",
    "    documents = [''] * len(users_ranking)\n",
    "    query = ['artificial inteligence']\n",
    "    \n",
    "    for i, n in enumerate(nodes_community):\n",
    "        labels = ''.join([str(e[2]['Label'] + ' ')*e[2]['Weight'] for e in G.edges(n, data=True)])\n",
    "        documents[i] += labels\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    D = vectorizer.fit_transform(documents)\n",
    "    Q = vectorizer.transform(query)\n",
    "    \n",
    "    sim = cosine_similarity(D, Q)\n",
    "    for i, n in enumerate(users_ranking.keys()):\n",
    "        users_ranking[n] = sim[i][0]\n",
    "    \n",
    "    # Add your code here\n",
    "    \n",
    "    return users_ranking\n",
    " \n",
    "profile_based_recommendation = profile_based_recommendation(G, nodes_community, communities, communities_count)\n",
    "sorted(profile_based_recommendation.items(), key=lambda x: x[1], reverse=True)[:5] if profile_based_recommendation != {n:0.0 for n in nodes_community.keys()} else None # prints sorted results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the ranking of the communities based on the importance of their nodes?\n",
    "- Compute a node-importance metric on the full graph. \n",
    "- Compute, for each community, the average of its nodes' importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('computer science', 0.001074198483014965),\n",
       " ('chemistry', 0.0007865879573917386),\n",
       " ('drones', 0.0007752379929126834),\n",
       " ('physicist', 0.0007371595247005386),\n",
       " ('pathways', 0.0007265159950794386),\n",
       " ('criminology', 0.0007248280212682204),\n",
       " ('cell', 0.000724781302183321),\n",
       " ('architecture', 0.0007096220351582247),\n",
       " ('patients', 0.000704826493460207),\n",
       " ('innovation', 0.0007036868991029613)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def node_importance_ranking(G, nodes_community, communities, communities_count):\n",
    "    ''' input: G:nx.Graph \n",
    "               nodes_community:{node_id:community_id}\n",
    "               communities:[community_ids]\n",
    "               community_count:int\n",
    "       output: communities_ranking:{community_id:ranking}\n",
    "\n",
    "    '''\n",
    "    communities_ranking = {c:0.0 for c in communities}\n",
    "\n",
    "    # Add your code here\n",
    "    pr = nx.pagerank(G)\n",
    "    for n in pr:\n",
    "        communities_ranking[nodes_community[n]] += pr[n]\n",
    "\n",
    "    for c in communities:\n",
    "        communities_ranking[c] /= list(nodes_community.values()).count(c)\n",
    "    \n",
    "    return communities_ranking\n",
    "\n",
    "ranking = node_importance_ranking(G, nodes_community, communities, communities_count)\n",
    "sorted(ranking.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the ranking of the communities based on their intra-connectivity?\n",
    "- Compute, for each community, the ratio of its actual edges to the edges of a clique with the same number of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('physicist', 0.19444444444444445),\n",
       " ('computer science', 0.14285714285714285),\n",
       " ('drones', 0.09090909090909091),\n",
       " ('infection', 0.0784313725490196),\n",
       " ('astronomy', 0.06666666666666667),\n",
       " ('neuroscience', 0.06666666666666667),\n",
       " ('diabetes', 0.0641025641025641),\n",
       " ('criminology', 0.058823529411764705),\n",
       " ('pathways', 0.058333333333333334),\n",
       " ('chemistry', 0.05263157894736842)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def intra_connectivity_ranking(G, nodes_community, communities, communities_count):\n",
    "    ''' input: G:nx.Graph \n",
    "               nodes_community:{node_id:community_id}\n",
    "               communities:[community_ids]\n",
    "               community_count:int\n",
    "       output: communities_ranking:{community_id:ranking}\n",
    "\n",
    "    '''\n",
    "    communities_ranking = {c:0.0 for c in communities}\n",
    "    \n",
    "    # Add your code here\n",
    "    for c in communities:\n",
    "        nodes = [n for n in G.nodes if nodes_community[n]==c]\n",
    "        graph = G.subgraph(nodes)\n",
    "        if len(nodes) > 1:\n",
    "            communities_ranking[c] = len(graph.edges()) / (len(graph.nodes()) * (len(graph.nodes())-1) / 2)\n",
    "        else:\n",
    "            communities_ranking[c] = 0.0\n",
    "    \n",
    "    return communities_ranking\n",
    "\n",
    "ranking = intra_connectivity_ranking(G, nodes_community, communities, communities_count)\n",
    "sorted(ranking.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "name": "_merged",
  "toc": {
   "base_numbering": "0",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
